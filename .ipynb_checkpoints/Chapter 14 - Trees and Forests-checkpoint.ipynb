{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14\n",
    "# Trees and Forests\n",
    "\n",
    "The basis of tree-based learners is the decision tree wherein a series of decision rules are chained. \n",
    "* https://www.stat.berkeley.edu/~breiman/RandomForests/\n",
    "\n",
    "## 14.1 Training a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction for [[5, 4, 3, 2]] is [1]\n",
      "predicted probabilities for the three classes: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "model = decisiontree.fit(features, targets)\n",
    "\n",
    "observation = [[5, 4, 3, 2]]\n",
    "print(\"the prediction for {} is {}\".format(observation, model.predict(observation)))\n",
    "print(\"predicted probabilities for the three classes: {}\".format(model.predict(observation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Decision tree learners attempt to find a decision rule that produces the greatest decrease in impurity at a node. While there are a number of measurements of impurity, by default `DecisionTreeClassifier` uses Gini impurity:\n",
    "$$\n",
    "G(t) = 1 - \\sum_{i=1}^c{p_i^2}\n",
    "$$\n",
    "where G(t) is the Gini impurity at node t and $p_i$ is the proportion of observations of class c at node t.\n",
    "\n",
    "This process of finding the decision rules that create splits to increase impurity is repeated recursively untill all leaf nodes are pure (i.e. contain only one class) or some abritary cut-off is reached\n",
    "\n",
    "We can change the `criterion` parameter to use a different impurity measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create decision tree classifier using entropy\n",
    "decisiontree_entropy = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "model_entropy = decisiontree_entropy.fit(features, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "* Decision Tree Learning, Princeton (https://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf)\n",
    "\n",
    "## Training a Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "decisiontree = DecisionTreeRegressor(random_state=0)\n",
    "model = decisiontree.fit(features, target)\n",
    "\n",
    "observation = [[0.02, 16]]\n",
    "model.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Decision tree regression works similarly to decision tree classification, however instead of reducing Gini impurity or entropy, potential splits are by default measure on how much they reduce mean squared error (MSE):\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat y_i)^2}\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true value of the target and $\\hat y_i$ is the predicted value.\n",
    "\n",
    "We can use the `criterion` parameter to select the desired measurement of split quality. For example we can construct a tree whose splits reduce mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decisiontree_mae = DecisionTreeRegressor(criterion=\"mae\", random_state=0)\n",
    "model_mae = decisiontree_mae.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "* Decision Tree Regression, scikit-learn (http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)\n",
    "* http://saedsayad.com/decision_tree_reg.htm\n",
    "\n",
    "## 14.3 Visualizing a Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 Training a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 Identifying Important Features in Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7 Selecting Important Features in Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.8 Handling Imabalnced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.9 Controlling Tree Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.10 Improving Performance Through Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.11 Evaluating Random Forests with Out-of-Bag Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning_cookbook]",
   "language": "python",
   "name": "conda-env-machine_learning_cookbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
